{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10-things-to-know-about-spacy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LahiruTjay/advanced-spacy/blob/master/10_things_to_know_about_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6McMEdn4-Bc",
        "colab_type": "text"
      },
      "source": [
        "# 10 Things to Know about spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGG3gdshJIT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68omgh6AKYWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1xK2FJ-KdO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "00a0a8e1-12c6-46ed-f51b-1f8b67109aa4"
      },
      "source": [
        "doc = nlp(u\"Success is not final. Failure is not fatal. It is the courage to continue that counts.\")\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success is not final.\n",
            "Failure is not fatal.\n",
            "It is the courage to continue that counts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r61Su2x4s-yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp.make_doc(u\"This is a sentence\")   # create a Doc from raw text\n",
        "for name, proc in nlp.pipeline:             # iterate over components in order\n",
        "    doc = proc(doc)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3AbA7lH-7Rw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2414fdd0-f37c-4c51-a82e-2b1a3abff3d8"
      },
      "source": [
        "print(nlp.pipe_names)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIJs_YxC_iio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d89aba60-79cc-41ed-d50b-88328292957d"
      },
      "source": [
        "def my_component(doc):\n",
        "    print(\"After tokenization, this doc has {} tokens.\".format(len(doc)))\n",
        "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
        "    if len(doc) < 10:\n",
        "        print(\"This is a pretty short document.\")\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(my_component, name=\"print_info\", last=True)\n",
        "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
        "doc = nlp(u\"This is a sentence.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner', 'print_info']\n",
            "After tokenization, this doc has 5 tokens.\n",
            "The part-of-speech tags are: ['DET', 'VERB', 'DET', 'NOUN', 'PUNCT']\n",
            "This is a pretty short document.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1DIg6qgwxpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9bea4348-1480-4041-ec6a-74e922647ec9"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u\"San Francisco considers banning sidewalk delivery robots\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "from spacy.tokens import Span\n",
        "\n",
        "doc = nlp(u\"FB is hiring a new VP of global policy\")\n",
        "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u\"ORG\"])]\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "San Francisco 0 13 GPE\n",
            "FB 0 2 ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E15F6DS3lbD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "b1b3af4d-4ae1-4108-daeb-b1e446deae4d"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "apple = doc[0]\n",
        "print(\"Fine-grained POS tag\", apple.pos_, apple.pos)\n",
        "print(\"Coarse-grained POS tag\", apple.tag_, apple.tag)\n",
        "print(\"Word shape\", apple.shape_, apple.shape)\n",
        "print(\"Alphanumeric characters?\", apple.is_alpha)\n",
        "print(\"Punctuation mark?\", apple.is_punct)\n",
        "\n",
        "billion = doc[10]\n",
        "print(\"Digit?\", billion.is_digit)\n",
        "print(\"Like a number?\", billion.like_num)\n",
        "print(\"Like an email address?\", billion.like_email)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fine-grained POS tag PROPN 96\n",
            "Coarse-grained POS tag NNP 15794550382381185553\n",
            "Word shape Xxxxx 16072095006890171862\n",
            "Alphanumeric characters? True\n",
            "Punctuation mark? False\n",
            "Digit? False\n",
            "Like a number? True\n",
            "Like an email address? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kctvTR9jBg7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ff3e9e29-b650-480c-e8bc-6829aa60b396"
      },
      "source": [
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
            "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U5iuMqm2Qr8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "edf0670c-755f-496b-9418-6f5a7d36e16c"
      },
      "source": [
        "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior emoji. It's outranking eggplant ðŸ‘ \")\n",
        "print(doc[0].text)          # 'Peach'\n",
        "print(doc[1].text)          # 'emoji'\n",
        "print(doc[-1].text)         # 'ðŸ‘'\n",
        "print(doc[17:19].text)      # 'outranking eggplant'\n",
        "noun_chunks = list(doc.noun_chunks)\n",
        "print(noun_chunks[0].text)  # 'Peach emoji'\n",
        "sentences = list(doc.sents)\n",
        "assert len(sentences) == 3\n",
        "print(sentences[1].text)    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peach\n",
            "emoji\n",
            "ðŸ‘\n",
            "outranking eggplant\n",
            "Peach emoji\n",
            "Peach is the superior emoji.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcrurqNq4dA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0d7aa227-3e7e-4541-fbc5-cec7d5514ee4"
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def set_sentiment(matcher, doc, i, matches):\n",
        "    doc.sentiment += 0.1\n",
        "\n",
        "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
        "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"ðŸ˜€\", \"ðŸ˜‚\", \"ðŸ¤£\", \"ðŸ˜\"]]\n",
        "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
        "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
        "\n",
        "doc = nlp(u\"A text about Google I/O ðŸ˜€ðŸ˜€\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]\n",
        "    print(string_id, span.text)\n",
        "print(\"Sentiment\", doc.sentiment)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleIO Google I/O\n",
            "HAPPY ðŸ˜€\n",
            "HAPPY ðŸ˜€ðŸ˜€\n",
            "HAPPY ðŸ˜€\n",
            "Sentiment 0.30000001192092896\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}